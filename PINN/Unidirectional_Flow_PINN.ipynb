{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df407427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch 1.13.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Using torch\", torch.__version__)\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "Pi = np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a882b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a very simple torch method to compute derivatives.\n",
    "def nth_derivative(f, wrt, n):\n",
    "    for i in range(n):\n",
    "        grads = grad(f, wrt, create_graph=True, allow_unused=True)[0]\n",
    "        f = grads\n",
    "        if grads is None:\n",
    "            print('bad grad')\n",
    "            return torch.tensor(0.)\n",
    "    return grads\n",
    "\n",
    "def flat(x):\n",
    "    m = x.shape[0]\n",
    "    return [x[i] for i in range(m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e2c0957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is a general Neural Network (fully connected, feed forward neural network), a.k.a Multi Layer Perceptron (MLP)\n",
    "# Our PINN will be represented by a neural network of this type\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.il  = nn.Linear(2,20)   # The input layer (2 nodes) is mapped to the first hidden layer (with 20 nodes)\n",
    "        self.hl1  = nn.Linear(20,20) # The first hidden layer (20 nodes) is mapped to the second hidden layer (with 20 nodes)\n",
    "        self.hl2 = nn.Linear(20,20)  # \"\"\n",
    "        self.hl3 = nn.Linear(20,20)  # \"\"\n",
    "        self.hl4 = nn.Linear(20,20)  # \"\"\n",
    "        self.hl5 = nn.Linear(20,20)  # \"\"\n",
    "        self.ol  = nn.Linear(20,1)   # The last hiddlen layer (20 nodes) is mapped to the output layer (with 1 node, which represents the scalar value of u in Burger's PDE)\n",
    "\n",
    "        self.tn  = nn.Tanh() # defining activation function\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Here we compose the various input/hidden/output layers together, with nonlinear activiation functions in between\n",
    "        # This composition makes up the neural network function\n",
    "        u = torch.cat((x, t), 1)\n",
    "        u = self.il(u)\n",
    "        u = self.hl1(self.tn(u))\n",
    "        u = self.hl2(self.tn(u))\n",
    "        u = self.hl3(self.tn(u))\n",
    "        u = self.hl4(self.tn(u))\n",
    "        u = self.hl5(self.tn(u))\n",
    "        u = self.ol(u)\n",
    "        return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f27638fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "h = 1.0\n",
    "mu = 0.6; rho = 0.5;\n",
    "N = 32;\n",
    "dpdx = -2.0; gx = 0.0;\n",
    "V1 = 0.0; V2 = 0.0;\n",
    "al = 0.51;\n",
    "nstep = 20000\n",
    "nu = mu/rho\n",
    "Dy = h/N;\n",
    "Dt = al*Dy*Dy/nu;\n",
    "\n",
    "def unidirectional_equation(y,t,model):    \n",
    "    u = model(y,t)\n",
    "    u_t = nth_derivative(flat(u), t, 1)\n",
    "    u_y = nth_derivative(flat(u), y, 1)\n",
    "    u_yy = nth_derivative(flat(u_y), y, 1)\n",
    "    f = -u_t - 1/rho*dpdx + nu*u_yy + gx\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "478ec386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BC_Data(data.Dataset):\n",
    "    def __init__(self, nt, ny):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            ny - number of points used to sample boundary conditions in y\n",
    "            nt - number of points used to sample boundary conditions in t\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nt = nt\n",
    "        self.ny = ny\n",
    "        self.generate_bc_data()\n",
    "\n",
    "    def generate_bc_data(self):\n",
    "        # u(y,t=0) = 0\n",
    "        y = np.linspace(0,1,self.ny)\n",
    "        y_bc = torch.from_numpy(y).float()\n",
    "        t_bc = torch.from_numpy(np.zeros(self.ny)).float()\n",
    "        u_bc = torch.from_numpy(np.zeros(self.ny)).float()\n",
    "\n",
    "        # u(y=0,t) = 0\n",
    "        y_bc = torch.cat((y_bc, torch.from_numpy(np.zeros(self.nt)).float()))\n",
    "        t_bc = torch.cat((t_bc, torch.from_numpy(np.linspace(0,1,self.nt)).float()))\n",
    "        u_bc = torch.cat((u_bc, torch.from_numpy(np.zeros(self.nt)).float()))\n",
    "\n",
    "        # u(y=1,t) = 0\n",
    "        y_bc = torch.cat((y_bc, torch.from_numpy(np.ones(self.nt)).float())).reshape(-1,1)\n",
    "        t_bc = torch.cat((t_bc, torch.from_numpy(np.linspace(0,1,self.nt)).float())).reshape(-1,1)\n",
    "        u_bc = torch.cat((u_bc, torch.from_numpy(np.zeros(self.nt)).float())).reshape(-1,1)\n",
    "\n",
    "        y_bc.requires_grad = True\n",
    "        t_bc.requires_grad = True\n",
    "        u_bc.requires_grad = False\n",
    "\n",
    "        self.y = y_bc\n",
    "        self.t = t_bc\n",
    "        self.u = u_bc\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    \n",
    "class RandCollocationPoints(data.Dataset):\n",
    "\n",
    "    def __init__(self, n_coll_pts):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            n_coll_pts - Number of randomly selected collocation points\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_coll_pts = n_coll_pts\n",
    "        self.generate_coll_points()\n",
    "\n",
    "    def generate_coll_points(self):\n",
    "    \n",
    "        y = np.random.uniform(0,1, self.n_coll_pts) # uniform random sampling of y between 0 and 1\n",
    "        t = np.random.uniform(0,1, self.n_coll_pts) # uniform random sampling of t between 0 and 1\n",
    "        \n",
    "        y = torch.from_numpy(y).float().reshape(-1,1)\n",
    "        t = torch.from_numpy(t).float().reshape(-1,1)\n",
    "        \n",
    "        y.requires_grad = True\n",
    "        t.requires_grad = True\n",
    "        \n",
    "        self.y = y\n",
    "        self.t = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f4a1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seeds so that the results can be exactly reproduced\n",
    "np.random.seed(88)\n",
    "torch.random.manual_seed(88)\n",
    "\n",
    "# Create Boundary condition data (object of the BC_Data class)\n",
    "bc_data = BC_Data(nt = 256, ny = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a384a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoc =  0\n",
      "loss =  15.83424186706543\n",
      "epoc =  500\n",
      "loss =  0.016987968236207962\n",
      "epoc =  1000\n",
      "loss =  0.010060738772153854\n",
      "epoc =  1500\n",
      "loss =  0.007065902929753065\n",
      "epoc =  2000\n",
      "loss =  0.006242668721824884\n",
      "epoc =  2500\n",
      "loss =  0.005548718385398388\n",
      "epoc =  3000\n",
      "loss =  0.004699537996202707\n"
     ]
    }
   ],
   "source": [
    "# Initialize nn_model\n",
    "pinn = MLP()\n",
    "\n",
    "# Create empty lists to track optimization progress of the boundary conditions (L_bc) + optimization of PDE (L_pde)\n",
    "L_bc_evol = []\n",
    "L_pde_evol = []\n",
    "\n",
    "# Initialize optimizer -- Setting pinn parameters as the parameters to be optimized. Starting with a learning rate lr = 0.001 (good rule of thumb)\n",
    "optimizer = optim.Adam(pinn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 3500 # This sets the number of optimization iterations \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    ###################################################################\n",
    "    # Evaluate discrepancy relative to the boundary conditions (L_bc)\n",
    "    ###################################################################\n",
    "    pinn_at_bc_pts = pinn(bc_data.y, bc_data.t) # evaluating pinn at boundary y and t values\n",
    "    L_bc = torch.mean(torch.pow(pinn_at_bc_pts - bc_data.u,2)) # computing mean squared error between pinn and boundary condition values\n",
    "    ###################################################################\n",
    "\n",
    "    \n",
    "    ###################################################################\n",
    "    # Evaluate discrepancy relative to the Burger's PDE (L_PDE)\n",
    "    ###################################################################\n",
    "    coll_pts = RandCollocationPoints(n_coll_pts = 400) # randomly select n_coll_pts = 400 in the domain y in [-1,1] and t in [0,1] to evaluate PDE discrepancy; these are called collocation points; a new set of random collocation points is generated at each optimization iteration\n",
    "    pinn_at_coll_pts = unidirectional_equation(torch.cat((bc_data.y,coll_pts.y)), torch.cat((bc_data.t,coll_pts.t)), pinn) # evaluate PDE residual at collocation points; here we concatenate the (y,t) values at the boundaries with the (y,t) values of the collocation points\n",
    "\n",
    "    L_pde = torch.mean(torch.pow(pinn_at_coll_pts,2)) # computing mean squared error of the PDE residuals\n",
    "    ###################################################################\n",
    "    \n",
    "    \n",
    "    ###################################################################\n",
    "    # Optimize PINN parameters\n",
    "    ###################################################################\n",
    "    L_bc_pde = L_bc + L_pde # Combine the boundary condition and PDE residuals\n",
    "    \n",
    "    # Now we optimize (always done by these 3 steps):\n",
    "    optimizer.zero_grad() # set all gradients previously calculated to zero\n",
    "    L_bc_pde.backward() # compute gradient of our objective function (that we want to minimize) with respect to the parameters of the PINN\n",
    "    optimizer.step() # update the parameters of the PINN according to gradient descent\n",
    "    ###################################################################\n",
    "    \n",
    "    # Append L_bc and L_pde values to track their evolution\n",
    "    L_bc_evol.append(L_bc.item())\n",
    "    L_pde_evol.append(L_pde.item())\n",
    "    \n",
    "    if (epoch % 500 == 0) or (epoch == num_epochs-1):\n",
    "        print('epoc = ', epoch)\n",
    "        print('loss = ', float(L_bc_pde))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05cbfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(L_bc_evol, label = r'$L_{BC}$')\n",
    "plt.semilogy(L_pde_evol, label = r'$L_{PDE}$')\n",
    "plt.semilogy(np.array(L_bc_evol) + np.array(L_pde_evol), label = r'$L_{BC}+L_{PDE}$')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('optimization iterations')\n",
    "plt.ylim(0.5e-3,)\n",
    "plt.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507ad138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the PINN solution:\n",
    "vtn = 64\n",
    "vyn = 128\n",
    "vy = np.linspace (0, 1, vyn )\n",
    "vt = np.linspace (0, 1, vtn )\n",
    "Vy, Vt = np.meshgrid(vy,vt)\n",
    "Vy = torch.FloatTensor(Vy.flatten().reshape(-1,1))\n",
    "Vt = torch.FloatTensor(Vt.flatten().reshape(-1,1))\n",
    "u = pinn(Vy,Vt).reshape(vtn,vyn)\n",
    "u = u.detach().numpy().T\n",
    "\n",
    "plt.imshow(u, origin = 'lower', extent = [0,1,0,1], aspect='auto')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.colorbar(label = 'u(y,t)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1906de60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(u[:,0], vy)\n",
    "plt.plot(u[:,-1], vy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
